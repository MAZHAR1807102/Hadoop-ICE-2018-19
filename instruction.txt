------------PLEASE MAKE SURE THAT YOUR SSH CONNECTION IS ESTABLISHED------------------

>> ssh localhost
>> ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa 
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
>> chmod 0600 ~/.ssh/authorized_keys
>> hadoop-3.4.0/bin/hdfs namenode -format
>> export PDSH_RCMD_TYPE=ssh
>> start-all.sh

---------------------ANSWER TO THE QUESTION NO. 1--------------------------------------

Create a folder in home named "WhatUWant"

Now open terminal and go to the created folder
Create a file named "data and last-3 digit of your roll"
>> touch data102.txt

Create a HDFS directory named "folder and last-3 digit of your roll"
>> hdfs dfs -mkdir /folder102

Upload the text file (ex:data102.txt) into the hdfs folder
>> hdfs dfs -put "location of data102.txt" /folder102 
(for me location of my file is : '/home/user/Desktop/Exam/data102.txt'

Write the mapreduce program which is provided in WordCount.java file. Be sure to put the java file in the same folder. Also create a folder in the folder "WhatUWant" named "classes" to store the manifest file and map() and reduce() class file.

Create a jar file named as "wordcount and last-3 digit of your roll"
>> export HADOOP_CLASSPATH=$(hadoop classpath)
>> echo $HADOOP_CLASSPATH
>> javac -classpath ${HADOOP_CLASSPATH} -d '/home/user/Desktop/WhatUwant/classes' '/home/user/Desktop/whatUwant/WordCount.java'
>> jar -cvf wordcount102.jar -C classes/ .   [This is the command for creating jar file]

Now run the program
>>hadoop jar '/home/user/Desktop/WhatUWant/wordcount102.jar' WordCount /folder102/ /folder102/Output

To see the output
>> hdfs dfs -cat /WordCount/Output/*


------------------------ANSWER TO THE QUESTION NO. 2-----------------------------------

>> cd
>> cd hive/bin

Upload the .csv file in HDFS directory which is asked to create named "folder and last 3 digit of your roll"
>> hdfs dfs -mkdir /folder102
>> hdfs dfs -put '/home/user/Desktop/Exam/FILE.csv' /folder102

Create Default directory of HIVE(Habse) in HDFS 
>> hdfs dfs -mkdir -p /user/hive/warehouse

Create database into /folder102 named "student and last 3 digit of your roll"
>> hive
>> create database if not exists student102 location '/folder102/';
>> use student102;

Create Table "Info and last three digit of your roll" partitioned by Gender FOR FORMAT DELIMITED FIELDS TERMINATED BY ',' Location '/folder102/Info'
>> create external table Info102(Id int, Name string, Age int) partitioned by(Gender string) row format delimited fields terminated by ',' location '/folder102/Info';
>> set hive.exec.dynamic.partition.mode=nonstrict;
>> load data inpath '/folder102' overwrite into table info102 partition(gender);

Open another terminal and check the files which were partitioned
>> hdfs dfs -ls /folder/Info/
>> hdfs dfs -cat /folder102/Info/gender=F/000000_0


Create a Table "temp" to store the data from csv
>> create external table temp(Id int, Name string, Roll int, Gender string) partitioned by(City string) row format delimited fields terminated by ',' location '/folder102/Adress';
>> set hive.exec.dynamic.partition.mode=nonstrict;
>> load data inpath '/folder102/FILE.csv' overwrite into table temp;
>> select * from temp;

Create Table "Adress and last three digit of your roll" partitioned by City FOR FORMAT DELIMITED FIELDS TERMINATED BY ',' Location '/folder102/Adress'
>> create external table Adress102(Id int) partitioned by(City string) row format delimited fields terminated by ',' location '/folder102/Adress';
>> INSERT OVERWRITE TABLE Adress102 PARTITION (City) SELECT ID,City FROM temp;

Open another terminal and check the files which were partitioned
>> hdfs dfs -ls /folder/Adress/
>> hdfs dfs -ls /folder102/Adress/
>> hdfs dfs -ls /folder102/Adress/city=K
>> hdfs dfs -cat /folder102/Adress/city=K/000000_0


